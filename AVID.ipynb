{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 0.4.1\n",
      "torchvision version: 0.2.1\n",
      "Is GPU available: True\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import skimage\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import make_grid, save_image\n",
    "\n",
    "print('PyTorch version:', torch.__version__)\n",
    "print('torchvision version:', torchvision.__version__)\n",
    "use_gpu = torch.cuda.is_available()\n",
    "print('Is GPU available:', use_gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general settings\n",
    "\n",
    "# device\n",
    "device = torch.device('cuda' if use_gpu else 'cpu')\n",
    "\n",
    "# batchsize (same as AVID paper)\n",
    "batchsize = 16\n",
    "\n",
    "# seed setting (warning : cuDNN's randomness is remaining)\n",
    "seed = 1\n",
    "torch.manual_seed(seed)\n",
    "if use_gpu:\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    \n",
    "# directory settings\n",
    "# Data directory (for IR-MNIST)\n",
    "data_dir = '../../data/IR-MNIST/'\n",
    "train_data_dir = data_dir + 'Train_Samples/'\n",
    "test_data_dir = data_dir + 'Test_Samples/'\n",
    "\n",
    "# directory to put generated images\n",
    "output_dir = data_dir + 'output/'\n",
    "if not os.path.exists(output_dir):\n",
    "    os.mkdir(output_dir)\n",
    "    \n",
    "# directory to save state_dict and loss.npy\n",
    "save_dir = data_dir + 'save/'\n",
    "if not os.path.exists(save_dir):\n",
    "    os.mkdir(save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make dataset class for image loading\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform = None):\n",
    "        self.root_dir = root_dir\n",
    "        self.list_dir = os.listdir(root_dir)\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.list_dir)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.root_dir + self.list_dir[idx]\n",
    "        image = skimage.io.imread(img_name)\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define transform\n",
    "# Normalize [0~255] to [-1~1]\n",
    "class Normalize:\n",
    "    def __call__(self, image):\n",
    "        return (image - 127.5) / 127.5\n",
    "    \n",
    "class Tofloat:\n",
    "    def __call__(self, tensor):\n",
    "        return tensor.float()\n",
    "    \n",
    "tf = transforms.Compose([Normalize(), transforms.ToTensor(), Tofloat()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of training data: 4000\n",
      "The number of validation data: 1000\n"
     ]
    }
   ],
   "source": [
    "# make dataset\n",
    "imgDataset = MyDataset(train_data_dir, transform = tf)\n",
    "\n",
    "# split to train data and validation data\n",
    "train_data, validation_data = train_test_split(imgDataset, test_size = 0.2, random_state = seed)\n",
    "\n",
    "print('The number of training data:', len(train_data))\n",
    "print('The number of validation data:', len(validation_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make DataLoader\n",
    "train_loader = DataLoader(train_data, batch_size = batchsize, shuffle = True)\n",
    "validation_loader = DataLoader(validation_data, batch_size = batchsize, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nX = iter(train_loader).next()[0].numpy()\\nX = np.transpose(X, [1, 2, 0]) * 0.5 + 0.5\\nprint(X.shape)\\nplt.imshow(X)\\n'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# visualization of an example of training data\n",
    "# comment out when runnnig in no GUI machine\n",
    "'''\n",
    "X = iter(train_loader).next()[0].numpy()\n",
    "X = np.transpose(X, [1, 2, 0]) * 0.5 + 0.5\n",
    "print(X.shape)\n",
    "plt.imshow(X)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define parts for U-net and FCN for convenience\n",
    "# downsampling\n",
    "# conv > batchnorm > dropout > leakyrelu\n",
    "class Downsample(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size = 4 , stride = 2, padding = 1, \\\n",
    "                                                                    use_batchnorm = True, use_dropout = False):\n",
    "        super(Downsample, self).__init__()\n",
    "        self.cv = nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding)\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "        self.dr = nn.Dropout(0.5)\n",
    "        self.rl = nn.LeakyReLU(0.2)\n",
    "        \n",
    "        self.use_batchnorm = use_batchnorm\n",
    "        self.use_dropout = use_dropout\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.cv(x)\n",
    "        \n",
    "        if self.use_batchnorm:\n",
    "            out = self.bn(out)\n",
    "            \n",
    "        if self.use_dropout:\n",
    "            out = self.dr(out)\n",
    "            \n",
    "        out = self.rl(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define parts for U-net for convenience\n",
    "# upsampling (using transposed convolution)\n",
    "# conv > batchnorm > dropout > relu\n",
    "class Upsample(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size = 4, stride = 2, padding = 1, \\\n",
    "                                                                   use_batchnorm = True, use_dropout = False):\n",
    "        super(Upsample, self).__init__()\n",
    "        self.tc = nn.ConvTranspose2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding)\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "        self.dr = nn.Dropout(0.5)\n",
    "        self.rl = nn.ReLU()\n",
    "        \n",
    "        self.use_batchnorm = use_batchnorm\n",
    "        self.use_dropout = use_dropout\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.tc(x)\n",
    "        \n",
    "        if self.use_batchnorm:\n",
    "            out = self.bn(out)\n",
    "            \n",
    "        if self.use_dropout:\n",
    "            out = self.dr(out)\n",
    "            \n",
    "        out = self.rl(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Memo : CNN size equation (no dilation)\n",
    "\n",
    "                                OUT = (IN + 2*Padding - Kernel_size) / Stride + 1                 \n",
    "'''\n",
    "# define Inpainter (Generator for GAN)\n",
    "# U-net architecture\n",
    "class Inpainter(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Inpainter, self).__init__()\n",
    "        \n",
    "        # U-net encoder\n",
    "        # default: kernel_size = 4, stride = 2, padding = 1, using batchnorm, no dropout\n",
    "        self.encoder1 = Downsample(  3,  32, use_batchnorm = False)   # out tensor size: (batchsize,  32, 112, 112)\n",
    "        self.encoder2 = Downsample( 32,  64)                          # out tensor size: (batchsize,  64,  56,  56)\n",
    "        self.encoder3 = Downsample( 64, 128)                          # out tensor size: (batchsize, 128,  28,  28)\n",
    "        self.encoder4 = Downsample(128, 256)                          # out tensor size: (batchsize, 256,  14,  14)\n",
    "        self.encoder5 = Downsample(256, 512)                          # out tensor size: (batchsize, 512,   7,   7)\n",
    "        \n",
    "        # U-net decoder\n",
    "        # default: kernel_size = 4, stride = 2, padding = 1, using batchnorm, no dropout\n",
    "        self.decoder1 = Upsample(512    , 512)                        # out tensor size: (batchsize, 512,  14,  14)\n",
    "        self.decoder2 = Upsample(512+256, 512)                        # out tensor size: (batchsize, 512,  28,  28)\n",
    "        self.decoder3 = Upsample(512+128, 256)                        # out tensor size: (batchsize, 256,  56,  56)\n",
    "        self.decoder4 = Upsample(256+ 64, 128)                        # out tensor size: (batchsize, 128, 112, 112)\n",
    "        self.decoder5 = Upsample(128+ 32,  64)                        # out tensor size: (batchsize,  64, 224, 224)\n",
    "        \n",
    "        # pointwise convolution to adjust channel with no image size change\n",
    "        self.decoder_final = nn.Conv2d(64, 3, kernel_size = 1, stride = 1, padding = 0)\n",
    "        self.th = nn.Tanh()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # encoding part\n",
    "        out_encoder1 = self.encoder1(x)\n",
    "        out_encoder2 = self.encoder2(out_encoder1)\n",
    "        out_encoder3 = self.encoder3(out_encoder2)\n",
    "        out_encoder4 = self.encoder4(out_encoder3)\n",
    "        out_encoder5 = self.encoder5(out_encoder4)\n",
    "        \n",
    "        # decording part\n",
    "        out_decoder1 = self.decoder1(out_encoder5)\n",
    "        out_decoder2 = self.decoder2(torch.cat([out_decoder1, out_encoder4], dim = 1))\n",
    "        out_decoder3 = self.decoder3(torch.cat([out_decoder2, out_encoder3], dim = 1))\n",
    "        out_decoder4 = self.decoder4(torch.cat([out_decoder3, out_encoder2], dim = 1))\n",
    "        out_decoder5 = self.decoder5(torch.cat([out_decoder4, out_encoder1], dim = 1))\n",
    "        \n",
    "        out = self.decoder_final(out_decoder5)\n",
    "        out = self.th(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Memo : CNN size equation (no dilation)\n",
    "\n",
    "                                OUT = (IN + 2*Padding - Kernel_size) / Stride + 1                 \n",
    "'''\n",
    "# define Detector (Discriminator for GAN)\n",
    "# FCN-architecture (PatchGAN discriminator)\n",
    "''' Issue : Should we condition discriminator by input Image? '''\n",
    "class Detector(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Detector, self).__init__()\n",
    "        # default: kernel_size = 4, stride = 2, padding = 1, using batchnorm, no dropout\n",
    "        self.fcn1x = Downsample(  3,  32, use_batchnorm = False)   # out tensor size: (batchsize,  32, 112, 112)\n",
    "        self.fcn1y = Downsample(  3,  32, use_batchnorm = False)   # out tensor size: (batchsize,  32, 112, 112)\n",
    "        self.fcn2  = Downsample( 64,  64)                          # out tensor size: (batchsize,  64,  56,  56)\n",
    "        self.fcn3  = Downsample( 64, 128)                          # out tensor size: (batchsize, 128,  28,  28)\n",
    "        self.fcn4  = Downsample(128, 256)                          # out tensor size: (batchsize, 256,  14,  14)\n",
    "        self.fcn5  = Downsample(256, 512, stride = 1, padding = 0) # out tensor size: (batchsize, 512,  11,  11)\n",
    "        \n",
    "        # pointwise convolution to adjust channel with no image size change\n",
    "        self.fcn_final = nn.Conv2d(512, 1, kernel_size = 1, stride = 1, padding = 0)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x, y):\n",
    "        # input x:Inpainter output, y:original image for conditioning\n",
    "        out_x = self.fcn1x(x)\n",
    "        out_y = self.fcn1y(y)\n",
    "        out = torch.cat([out_x, out_y], dim = 1)\n",
    "        \n",
    "        out = self.fcn2(out)\n",
    "        out = self.fcn3(out)\n",
    "        out = self.fcn4(out)\n",
    "        out = self.fcn5(out)\n",
    "        \n",
    "        out = self.fcn_final(out)\n",
    "        out = self.sigmoid(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# network, optimizer and hyperparameters settings\n",
    "\n",
    "# instantiate networks\n",
    "inpainter = Inpainter()\n",
    "detector = Detector()\n",
    "\n",
    "# send to GPU(CPU)\n",
    "inpainter = inpainter.to(device)\n",
    "detector = detector.to(device)\n",
    "\n",
    "# set optimizer\n",
    "inp_optimizer = optim.Adam(inpainter.parameters(), lr = 0.0002, betas = [0.5, 0.999])\n",
    "det_optimizer = optim.Adam(detector.parameters(), lr = 0.0002, betas = [0.5, 0.999])\n",
    "\n",
    "# init weights\n",
    "for p in inpainter.parameters():\n",
    "    nn.init.normal_(p, mean = 0, std = 0.02)\n",
    "    \n",
    "for p in detector.parameters():\n",
    "    nn.init.normal_(p, mean = 0, std = 0.02)\n",
    "\n",
    "# count the number of trainable parameters\n",
    "num_trainable_params_inp = sum(p.numel() for p in inpainter.parameters() if p.requires_grad)\n",
    "num_trainable_params_det = sum(p.numel() for p in detector.parameters() if p.requires_grad)\n",
    "\n",
    "# coefficient for gaussian noise to add to inpainter input\n",
    "γ = 0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------------\n",
      "Inpainter\n",
      "The number of trainable parameters: 16720803\n",
      "\n",
      "Model\n",
      " Inpainter(\n",
      "  (encoder1): Downsample(\n",
      "    (cv): Conv2d(3, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (dr): Dropout(p=0.5)\n",
      "    (rl): LeakyReLU(negative_slope=0.2)\n",
      "  )\n",
      "  (encoder2): Downsample(\n",
      "    (cv): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (dr): Dropout(p=0.5)\n",
      "    (rl): LeakyReLU(negative_slope=0.2)\n",
      "  )\n",
      "  (encoder3): Downsample(\n",
      "    (cv): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (dr): Dropout(p=0.5)\n",
      "    (rl): LeakyReLU(negative_slope=0.2)\n",
      "  )\n",
      "  (encoder4): Downsample(\n",
      "    (cv): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (dr): Dropout(p=0.5)\n",
      "    (rl): LeakyReLU(negative_slope=0.2)\n",
      "  )\n",
      "  (encoder5): Downsample(\n",
      "    (cv): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (dr): Dropout(p=0.5)\n",
      "    (rl): LeakyReLU(negative_slope=0.2)\n",
      "  )\n",
      "  (decoder1): Upsample(\n",
      "    (tc): ConvTranspose2d(512, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (dr): Dropout(p=0.5)\n",
      "    (rl): ReLU()\n",
      "  )\n",
      "  (decoder2): Upsample(\n",
      "    (tc): ConvTranspose2d(768, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (dr): Dropout(p=0.5)\n",
      "    (rl): ReLU()\n",
      "  )\n",
      "  (decoder3): Upsample(\n",
      "    (tc): ConvTranspose2d(640, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (dr): Dropout(p=0.5)\n",
      "    (rl): ReLU()\n",
      "  )\n",
      "  (decoder4): Upsample(\n",
      "    (tc): ConvTranspose2d(320, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (dr): Dropout(p=0.5)\n",
      "    (rl): ReLU()\n",
      "  )\n",
      "  (decoder5): Upsample(\n",
      "    (tc): ConvTranspose2d(160, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (dr): Dropout(p=0.5)\n",
      "    (rl): ReLU()\n",
      "  )\n",
      "  (decoder_final): Conv2d(64, 3, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (th): Tanh()\n",
      ")\n",
      "\n",
      "Optimizer\n",
      " Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: [0.5, 0.999]\n",
      "    eps: 1e-08\n",
      "    lr: 0.0002\n",
      "    weight_decay: 0\n",
      ")\n",
      "--------------------------------------------------------------------------------------\n",
      "Detector\n",
      "The number of trainable parameters: 2824705\n",
      "\n",
      "Model\n",
      " Detector(\n",
      "  (fcn1x): Downsample(\n",
      "    (cv): Conv2d(3, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (dr): Dropout(p=0.5)\n",
      "    (rl): LeakyReLU(negative_slope=0.2)\n",
      "  )\n",
      "  (fcn1y): Downsample(\n",
      "    (cv): Conv2d(3, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (dr): Dropout(p=0.5)\n",
      "    (rl): LeakyReLU(negative_slope=0.2)\n",
      "  )\n",
      "  (fcn2): Downsample(\n",
      "    (cv): Conv2d(64, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (dr): Dropout(p=0.5)\n",
      "    (rl): LeakyReLU(negative_slope=0.2)\n",
      "  )\n",
      "  (fcn3): Downsample(\n",
      "    (cv): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (dr): Dropout(p=0.5)\n",
      "    (rl): LeakyReLU(negative_slope=0.2)\n",
      "  )\n",
      "  (fcn4): Downsample(\n",
      "    (cv): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (dr): Dropout(p=0.5)\n",
      "    (rl): LeakyReLU(negative_slope=0.2)\n",
      "  )\n",
      "  (fcn5): Downsample(\n",
      "    (cv): Conv2d(256, 512, kernel_size=(4, 4), stride=(1, 1))\n",
      "    (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (dr): Dropout(p=0.5)\n",
      "    (rl): LeakyReLU(negative_slope=0.2)\n",
      "  )\n",
      "  (fcn_final): Conv2d(512, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n",
      "\n",
      "Optimizer\n",
      " Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: [0.5, 0.999]\n",
      "    eps: 1e-08\n",
      "    lr: 0.0002\n",
      "    weight_decay: 0\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# print settings\n",
    "\n",
    "print('--------------------------------------------------------------------------------------')\n",
    "print('Inpainter')\n",
    "print('The number of trainable parameters:', num_trainable_params_inp)\n",
    "print('\\nModel\\n', inpainter)\n",
    "print('\\nOptimizer\\n', inp_optimizer)\n",
    "\n",
    "print('--------------------------------------------------------------------------------------')\n",
    "print('Detector')\n",
    "print('The number of trainable parameters:', num_trainable_params_det)\n",
    "print('\\nModel\\n', detector)\n",
    "print('\\nOptimizer\\n', det_optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define loss function\n",
    "def inp_loss(det_output_fake):\n",
    "    adversarial_loss = F.binary_cross_entropy(det_output_fake, torch.ones_like(det_output_fake))\n",
    "    return adversarial_loss\n",
    "\n",
    "def det_loss(det_output_real, det_output_fake):\n",
    "    real_loss = F.binary_cross_entropy(det_output_real, torch.ones_like(det_output_real))\n",
    "    fake_loss = F.binary_cross_entropy(det_output_fake, torch.zeros_like(det_output_fake))\n",
    "    adversarial_loss = 0.5 * (real_loss + fake_loss) # devide by 2 to regulate learning speed (mentioned by pix2pix)\n",
    "    return adversarial_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(data_loader):\n",
    "    inpainter.train()\n",
    "    detector.train()\n",
    "    \n",
    "    running_inpainter_loss = 0\n",
    "    running_detector_loss = 0\n",
    "    \n",
    "    for inputs in data_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        # make contaminated inputs for inpainter network by adding gaussian noise\n",
    "        inputs_contaminated = inputs + γ * torch.randn(inputs.size()).to(device)\n",
    "        \n",
    "        # calculate network outputs\n",
    "        inp_outputs = inpainter(inputs_contaminated)\n",
    "        det_outputs_real = detector(inputs, inputs)\n",
    "        # .detach() is used not to track inpainter gradient during detector backward calculation\n",
    "        det_outputs_fake_for_det = detector(inp_outputs.detach(), inputs)\n",
    "        det_outputs_fake_for_inp = detector(inp_outputs, inputs)\n",
    "        \n",
    "        # detector : calculate loss function, run backward calculation, and update weights\n",
    "        det_optimizer.zero_grad()\n",
    "        detector_loss = det_loss(det_outputs_real, det_outputs_fake_for_det)\n",
    "        detector_loss.backward()\n",
    "        det_optimizer.step()\n",
    "        \n",
    "        # inpainter : calculate loss function, run backward calculation, and update weights\n",
    "        inp_optimizer.zero_grad()\n",
    "        inpainter_loss = inp_loss(det_outputs_fake_for_inp)\n",
    "        inpainter_loss.backward()\n",
    "        inp_optimizer.step()\n",
    "        \n",
    "        # extract number from zero dimension tensor by .item()\n",
    "        running_inpainter_loss += inpainter_loss.item()\n",
    "        running_detector_loss += detector_loss.item()\n",
    "        \n",
    "    # devide by len(data_loader) because F.binary_cross_entropy normalize loss in minibatch\n",
    "    average_inpainter_loss = running_inpainter_loss / len(data_loader)\n",
    "    average_detector_loss = running_detector_loss / len(data_loader)\n",
    "    \n",
    "    return average_inpainter_loss, average_detector_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_save_images = 5\n",
    "interval_save_images = 5 # epoch\n",
    "\n",
    "def validation(data_loader, epoch):\n",
    "    inpainter.train()\n",
    "    detector.train()\n",
    "    \n",
    "    running_inpainter_loss = 0\n",
    "    running_detector_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs in data_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            inputs_contaminated = inputs + γ * torch.randn(inputs.size()).to(device)\n",
    "\n",
    "            # calculate network outputs\n",
    "            # some code that needn't for validation is deleted from train()\n",
    "            inp_outputs = inpainter(inputs_contaminated)\n",
    "            det_outputs_real = detector(inputs, inputs)\n",
    "            det_outputs_fake = detector(inp_outputs, inputs)\n",
    "            \n",
    "            running_detector_loss += det_loss(det_outputs_real, det_outputs_fake).item()\n",
    "            running_inpainter_loss += inp_loss(det_outputs_fake).item()\n",
    "            \n",
    "    # save [n_save_images] (normal image, contaminated image, inpainted image) comparison image\n",
    "    if epoch % interval_save_images == 0:\n",
    "        for n in range(n_save_images):\n",
    "            normal_image = inputs[n].unsqueeze(0)\n",
    "            contaminated_image = inputs_contaminated[n].unsqueeze(0)\n",
    "            inpainted_image = inp_outputs[n].unsqueeze(0)\n",
    "            comparison = torch.cat([normal_image, contaminated_image, inpainted_image])\n",
    "            save_image(comparison.data.cpu(), '{}/{}_{}.png'.format(output_dir, epoch, n))\n",
    "                    \n",
    "    average_inpainter_loss = running_inpainter_loss / len(data_loader)\n",
    "    average_detector_loss = running_detector_loss / len(data_loader)\n",
    "    \n",
    "    return average_inpainter_loss, average_detector_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 3\n",
    "train_loss_list = [[], []]\n",
    "validation_loss_list = [[], []]\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    train_inp_loss, train_det_loss = train(train_loader)\n",
    "    validation_inp_loss, validation_det_loss = validation(validation_loader, epoch)\n",
    "    \n",
    "    train_loss_list[0].append(train_inp_loss)\n",
    "    train_loss_list[1].append(train_det_loss)\n",
    "    validation_loss_list[0].append(validation_inp_loss)\n",
    "    validation_loss_list[1].append(validation_det_loss)\n",
    "    \n",
    "    print('epoch[%d/%d] losses[train_inp:%1.4f train_det:%1.4f  validation_inp:%1.4f validation_det:%1.4f]' \\\n",
    "                        % (epoch+1, n_epochs, train_inp_loss, train_det_loss, validation_inp_loss, validation_det_loss)) \n",
    "\n",
    "\n",
    "# save state_dicts\n",
    "torch.save(inpainter.state_dict(), save_dir + 'inpainter_' + str(epoch) + '.pth')\n",
    "torch.save(detector.state_dict(), save_dir + 'detector_' + str(epoch) + '.pth')\n",
    "torch.save(inp_optimizer.state_dict(), save_dir + 'inp_optmizer_' + str(epoch) + '.pth')\n",
    "torch.save(det_optimizer.state_dict(), save_dir + 'det_discriminator' + str(epoch) + '.pth')\n",
    "\n",
    "# save learning log\n",
    "np.save(save_dir + 'train_loss_list.npy', np.array(train_loss_list))\n",
    "np.save(save_dir + 'validation_loss_list.npy', np.array(validation_loss_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
